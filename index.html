<html>
<head>
<title>DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation</title>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='./css/paperstyle.css' rel='stylesheet' type='text/css'>
</head>

<body>

<div class="pageTitle">
    DualAfford: Learning Collaborative Visual Affordance <br> for Dual-gripper Object Manipulation
  <br>
  <br>
  <span class = "Authors">
      <a href="https://sxy7147.github.io/" target="_blank">Yan Zhao<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://fqnchina.github.io/" target="_blank">Zhehuan Chen</a><sup>1</sup> &nbsp; &nbsp;
      <a href="https://www.linkedin.com/in/yourong-zhang-2b1aab23a/" target="_blank">Yourong Zhang</a><sup>1</sup> &nbsp; &nbsp; <br>
      <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a><sup>3</sup> &nbsp; &nbsp;
      <a href="https://www.cs.stanford.edu/~kaichun/" target="_blank">Kaichun Mo</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>
      <i>(*: indicates joint first authors)</i><br><br>
      <sup>1</sup><a href = "http://english.pku.edu.cn/" target="_blank"> Peking University </a> &nbsp; &nbsp;
      <sup>2</sup><a href = "http://www.stanford.edu/" target="_blank"> Stanford University </a> &nbsp; &nbsp;
      <sup>3</sup><a href = "https://ai.tencent.com/ailab/zh/index" target="_blank"> Tencent AI Lab </a> &nbsp; &nbsp;<br><br>
  </span>
  </div>
<br>
<div class = "material">
        <a href="https://iclr.cc/Conferences/2023" target="_blank">International Conference on Learning Representations (ICLR) 2023</a>
</div>
<br>
<br>
<div class = "material">
        <a href="https://arxiv.org/pdf/2207.01971.pdf" target="_blank">[Paper]</a> &nbsp; &nbsp;
        <a target="_blank">[Code](Coming Soon!)</a> &nbsp; &nbsp;
        <!-- <a href="https://github.com/warshallrho/VAT-Mart" target="_blank">[Code]</a> -->
        <a href="paper.bib" target="_blank">[BibTex]</a> &nbsp; &nbsp;
</div>

<div class = "abstractTitle">
  Abstract
</div>
<p class = "abstractText">
    It is essential yet challenging for future home-assistant robots to understand and manipulate diverse 3D objects in daily human environments. Towards building scalable systems that can perform diverse manipulation tasks over various 3D shapes, recent works have advocated and demonstrated promising results learning visual actionable affordance, which labels every point over the input 3D geometry with an action likelihood of accomplishing the downstream task (e.g., pushing or picking-up). However, these works only studied single-gripper manipulation tasks, yet many real-world tasks require two hands to achieve collaboratively. In this work, we propose a novel learning framework, DualAfford, to learn collaborative affordance for dual-gripper manipulation tasks. The core design of the approach is to reduce the quadratic problem for two grippers into two disentangled yet inter- connected subtasks for efficient learning. Using the large-scale PartNet-Mobility and ShapeNet datasets, we set up four benchmark tasks for dual-gripper manipulation. Experiments prove the effectiveness and superiority of our method over three baselines.
</p>

<br>
<div class="abstractTitle">
    Video Presentation
</div>
<br>
<center>
    <iframe width="660" height="415" src="https://www.youtube.com/embed/3NsnIIrgv0w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br><br><br>
    <iframe width="660" height="415" src="//player.bilibili.com/player.html?aid=258222312&bvid=BV1Ua411D7Sa&cid=974991223&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</center>

<br>
<br>
<br>

<div class="abstractTitle">
    Tasks
</div>
  <img class="bannerImage" src="./images/teaser.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 1.
        Given different shapes and manipulation tasks (e.g., pushing the keyboard in the direction indicated by the red arrow), 
        our proposed DualAfford framework predicts dual collaborative visual actionable affordance and gripper orientations. 
        The prediction for the second gripper (b) is dependent on the first (a). 
        We can directly apply our network to real-world data.
  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Framework and Network Architecture
</div>
  <img class="bannerImage" src="./images/pipeline2.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 2.
        Our proposed DualAfford framework, first collects interaction data points in physics simulation, then uses them to train the Perception Module, which contains the First Gripper Module and the Second Gripper Module, and further enhances the cooperation between two grippers through the Collaborative Adaption procedure. The training and the inference procedures, as respectively indicated by the red and blue arrows, share the same architecture but with opposite dataflow directions.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/dual_gripper_teaser_ICLR.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 3.
        Architecture details of the Perception Module. Given a 3D partial scan and a specific select task, 
        our network sequentially predicts the first and second grippers’ affordance maps and manipulation actions in a conditional manner. 
        Each gripper module is composed of 
        1) an Affordance Network A indicating where to interact; 
        2) a Proposal Network P suggesting how to interact; 
        3) a Critic Network C evaluating the success likelihood of an interaction.
  </p></td></tr></tbody></table>

  
<div class="abstractTitle">
    Qualitative Results
</div>
  <img class="bannerImage" src="./images/aff.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 4.
        Qualitative results of Affordance Networks. In each block, we respectively show 
        (1) task represented by a red arrow, 
        (2) object which should be moved from transparent to solid, 
        (3) the first affordance map predicted by A1, 
        (4) the second affordance map predicted by A2 conditioned on the first action. 
        Left shapes are from training categories, while right shapes are from unseen categories.

  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/critic.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 5.
        The per-point action scores predicted by Critic Networks C1 and C2.
        In each result block, from left to right, we show the task, the input shape, 
        the per-point success likelihood predicted by C1 given the first gripper orientation, 
        and the per-point success likelihood predicted by C2 given the second gripper orientation, conditioned on the first gripper’s action.

  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/ablation.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 6.
        (a) The diverse and collaborative actions proposed by the Proposal Networks P1 and P2. 
        (b) The promising results testing on real-world data. 
        (c) The actionable affordance maps of the ablated version that removes the Collaborative Adaptation procedure (left) and ours (right).
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/real.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
        Figure 7.
        We present some promising results by directly testing our model on real-world scans. 
        In each block, from left to right, we respectively show the task represented by a red arrow, the input shape, 
        the actionability heatmap predicted by network A1, 
        and the actionability heatmap predicted by network A2 conditioned on the action of the first gripper. 
        Please check our video for more results.
  </p></td></tr></tbody></table>


<!-- <p></p>  -->



<div class = "abstractTitle">
  Citation
</div>

<!-- <div class="row" style="margin-top: 1em">
<div class="12u$ 1u$(xsmall)">   -->
<div class = "abstractText">
  <pre style="background-color: #c0c0b8;">
  <code>@inproceedings{zhao2023dualafford,
    title={DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Manipulation},
    author={Yan Zhao and Ruihai Wu and Zhehuan Chen and Yourong Zhang and Qingnan Fan and Kaichun Mo and Hao Dong},
    booktitle={International Conference on Learning Representations},
    year={2023},
  }
</code></pre>
</div>
<!-- </div> -->


<br>
<br>


<div class = "abstractTitle">
  Contact
</div>
<p class = "abstractText">
  If you have any questions, please feel free to contact <a href="https://sxy7147.github.io/" target="_blank">Yan Zhao</a> at zhaoyan_790_at_pku_edu_cn.
</p>



<br>
<br>


<div class = "ackTitle">
  Acknowledgements
</div>
<p class = "ackText">
  This work was supported by National Natural Science Foundation of China (No. 62136001).
</p>


</body></html>